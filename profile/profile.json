{
  "education": [
    {
      "awards_achievements": [
        "Dean's List 2019 - 2020",
        "Joseph Lau Student Exchange Awards",
        "Hong Kong Hua-Yan Buddhist Association Exchange Scholarships",
        "GPA: 3.54/4.3"
      ],
      "degree": "Bachelor of Business Administration",
      "department": "Department of Management",
      "duration": "2016-2020",
      "school_name": "City University of Hong Kong"
    }
  ],
  "github_repo_root_url": "https://github.com/Jarvis-Consulting-Group/jarvis_data_eng-alextjin",
  "highlighted_projects": [
    {
      "description": "I built a reusable scripts to handle XML to CSV conversion. lxml is adopted for parsing the data. The script can support parsing the data from different root and layer. All the parent and child data would be captured.",
      "name": "XML to CSV conversion"
    },
    {
      "description": "I was responsible to build data pipelines for the middle office book keeping. We retrieved data from our vendor via APIs request. I utilised AWS s3, Airflow and Spark for data ingestion, transformation and loading phases. Adopted Great Expectation for data quality checking.",
      "name": "IBOR (Investment Book of Record) Data Package"
    },
    {
      "description": "I played a significant role in an on-prem-to-cloud project that involved integrating new data sources. I actively contributed by establishing connections with new data sources and constructing the project-specific database. This involved creating over 650 tables to facilitate efficient data storage and retrieval. Additionally, I designed and developed more than 20 ETL workflows using Informatica, ensuring smooth data transformations and transfers. To optimize operations, I leveraged Airflow and worked on three DAGs (Directed Acyclic Graphs) for workflow orchestration. Furthermore, I implemented bash scripts to automate tasks and monitor the performance of these workflows, enabling streamlined operations and efficient performance monitoring.",
      "name": "RISE"
    },
    {
      "description": "I built a library system management app designed to manage all the functions of a library. Users can do all the CRUD operations for books. The frontend is designed with Angular and the backend is held by Express Web Framework. I also adopted Angular Material, MySQL to enhance the web functionality and data storage. Unit test has benn conduct with Jest.",
      "git_url": null,
      "name": "Library System Management App"
    }
  ],
  "jarvis_projects": [
    {
      "description": "Reduced the workload of the Linux Cluster Administration (LCA) team by developing several scripts that track instance performance. I used Docker in implementation and testing to ensure consistent execution. The 'psql_docker.sh' script facilitates container creation and activation. 'host_info.sh' and 'host_usage.sh' extract platform details, with the latter scheduled using crontab for continuous data collection, minimizing manual intervention. All data is stored in a PSQL instance for future analytics. Git was utilized as the version control tool, with all project scripts accessible on GitHub for collaboration and transparency.",
      "git_url": "https://github.com/Jarvis-Consulting-Group/jarvis_data_eng-alextjin/tree/master/linux_sql",
      "name": "Cluster Monitor"
    }
  ],
  "name": "Alex Tjin",
  "others": [
    {
      "bullets": [
        "Microsoft Certified: Azure Data Fundamentals",
        "IBM Agile Explorer"
      ],
      "title": "Certificates"
    },
    {
      "bullets": [
        "Basketball player",
        "Hodophile"
      ],
      "title": "Activities/Hobbies"
    }
  ],
  "professional_experience": [
    {
      "company": "RBC",
      "description": "Played a pivotal role contributing to both Full Stack Development and Data Engineering initiatives. Being full responsible for comprehensive Data Pipeline development, from Data Modeling to report generation in Power BI, utilizing Python, Spark, and Airflow. Also, contributed in the migration from on-premises to cloud with Azure cloud services like ADF and Databricks.",
      "duration": "2023-present",
      "title": "Full Stack Developer"
    },
    {
      "company": "Jarvis",
      "description": "Engaged in various projects and bolstered my skills and proficiency in data engineering, encompassing the area of SQL, Python, Spark and Linux.",
      "duration": "2023-present",
      "title": "Full Stack Developer"
    },
    {
      "company": "IBM",
      "description": "Leveraged tools like Informatica, Apache Airflow, and AWS services to architect scalable and efficient data integration workflows, encompassed ingesting data from various sources (e.g csv, json, xml), employing Python in data cleansing and transformation, automated routine tasks in Bash. Also, maintained data warehouse integrity and swiftly rectify defects. Built report with IBM Cognos.",
      "duration": "2021-2023",
      "title": "Data Engineer"
    }
  ],
  "skills": {
    "competent": [
      "Javascript",
      "Typescript",
      "Node",
      "Angular",
      "REST API",
      "Bash"
    ],
    "familiar": [
      "VBA",
      "Data Visualisation",
      "Regression Testing",
      "Hadoop",
      "GCP"
    ],
    "proficient": [
      "SQL",
      "Python",
      "Spark",
      "Airflow",
      "Databricks",
      "Azure Data Factory",
      "AWS",
      "Azure Cloud Services",
      "Data Modelling",
      "Git",
      "Docker"
    ]
  },
  "summary": "As a dedicated Data Engineer, I bring over four years of comprehensive IT development experience to the table. My expertise primarily revolves around data ingestion and integration, where I excel in leveraging Python, SQL, Spark and Azure to seamlessly handle data workflows. I possess extensive knowledge of diverse databases, including Redshift, Oracle, and Postgres, allowing me to efficiently manage and manipulate data at scale. Furthermore, I have honed my skills in constructing robust Data Pipelines utilizing cutting-edge ETL tools like Informatica and Airflow. These tools enable me to orchestrate and automate complex data processes, ensuring streamlined data integration and efficient information flow. Also, I have acquired valuable exposure and hands-on experience with the three major cloud service providers AWS, Azure, and GCP, which enable me to design and implement scalable and performant data solutions that leverage the power and versatility of these platforms."
}
